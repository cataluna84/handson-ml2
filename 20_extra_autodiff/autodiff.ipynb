{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:34.181987Z",
     "iopub.status.busy": "2021-03-19T01:22:34.181398Z",
     "iopub.status.idle": "2021-03-19T01:22:34.183269Z",
     "shell.execute_reply": "2021-03-19T01:22:34.183646Z"
    },
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Introduction to gradients and automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6P32iYYV27b"
   },
   "source": [
    "## Automatic Differentiation and Gradients\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "is useful for implementing machine learning algorithms such as\n",
    "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n",
    "neural networks.\n",
    "\n",
    "In this guide, you will explore ways to compute gradients with TensorFlow, especially in [eager execution](eager.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:34.191632Z",
     "iopub.status.busy": "2021-03-19T01:22:34.191017Z",
     "iopub.status.idle": "2021-03-19T01:22:41.085639Z",
     "shell.execute_reply": "2021-03-19T01:22:41.085098Z"
    },
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Computing gradients\n",
    "\n",
    "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CLWJl0QliB0"
   },
   "source": [
    "## Gradient tapes\n",
    "\n",
    "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n",
    "TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:41.090025Z",
     "iopub.status.busy": "2021-03-19T01:22:41.089318Z",
     "iopub.status.idle": "2021-03-19T01:22:50.217391Z",
     "shell.execute_reply": "2021-03-19T01:22:50.216802Z"
    },
    "id": "Xq9GgTCP7a4A"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR9tFAP_7cra"
   },
   "source": [
    "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.225986Z",
     "iopub.status.busy": "2021-03-19T01:22:50.225380Z",
     "iopub.status.idle": "2021-03-19T01:22:50.234791Z",
     "shell.execute_reply": "2021-03-19T01:22:50.235183Z"
    },
    "id": "LsvrwF6bHroC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2_aqsO25Vx1"
   },
   "source": [
    "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.241951Z",
     "iopub.status.busy": "2021-03-19T01:22:50.241342Z",
     "iopub.status.idle": "2021-03-19T01:22:50.632250Z",
     "shell.execute_reply": "2021-03-19T01:22:50.631662Z"
    },
    "id": "vacZ3-Ws5VdV"
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eXOkrQ-9Pb"
   },
   "source": [
    "To get the gradient of `y` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.636966Z",
     "iopub.status.busy": "2021-03-19T01:22:50.636337Z",
     "iopub.status.idle": "2021-03-19T01:22:50.642485Z",
     "shell.execute_reply": "2021-03-19T01:22:50.642025Z"
    },
    "id": "luOtK1Da_BR0"
   },
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei4iVXi6qgM7"
   },
   "source": [
    "The gradient with respect to each source has the shape of the source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.646836Z",
     "iopub.status.busy": "2021-03-19T01:22:50.645901Z",
     "iopub.status.idle": "2021-03-19T01:22:50.648937Z",
     "shell.execute_reply": "2021-03-19T01:22:50.649350Z"
    },
    "id": "aYbWRFPZqk4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(dl_dw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI_SzxHsvao1"
   },
   "source": [
    "Here is the gradient calculation again, this time passing a dictionary of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.653735Z",
     "iopub.status.busy": "2021-03-19T01:22:50.653134Z",
     "iopub.status.idle": "2021-03-19T01:22:50.658299Z",
     "shell.execute_reply": "2021-03-19T01:22:50.657830Z"
    },
    "id": "d73cY6NOuaMd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.0213387, 2.5680232], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2LvHifEMgO"
   },
   "source": [
    "## Gradients with respect to a model\n",
    "\n",
    "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
    "\n",
    "In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.663214Z",
     "iopub.status.busy": "2021-03-19T01:22:50.662558Z",
     "iopub.status.idle": "2021-03-19T01:22:50.682905Z",
     "shell.execute_reply": "2021-03-19T01:22:50.682350Z"
    },
    "id": "JvesHtbQESc-"
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.687173Z",
     "iopub.status.busy": "2021-03-19T01:22:50.686433Z",
     "iopub.status.idle": "2021-03-19T01:22:50.689459Z",
     "shell.execute_reply": "2021-03-19T01:22:50.689010Z"
    },
    "id": "PR_ezr6UFrpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Gx6LS714zR"
   },
   "source": [
    "<a id=\"watches\"></a>\n",
    "\n",
    "## Controlling what the tape watches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4VlqKFzzGaC"
   },
   "source": [
    "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
    "\n",
    "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.695152Z",
     "iopub.status.busy": "2021-03-19T01:22:50.694566Z",
     "iopub.status.idle": "2021-03-19T01:22:50.699910Z",
     "shell.execute_reply": "2021-03-19T01:22:50.699359Z"
    },
    "id": "Kj9gPckdB37a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkcpQnLgNxgi"
   },
   "source": [
    "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.704452Z",
     "iopub.status.busy": "2021-03-19T01:22:50.703697Z",
     "iopub.status.idle": "2021-03-19T01:22:50.706488Z",
     "shell.execute_reply": "2021-03-19T01:22:50.706887Z"
    },
    "id": "hwNwjW1eAkib"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9I1uFvB4tf"
   },
   "source": [
    "`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n",
    "\n",
    "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.711808Z",
     "iopub.status.busy": "2021-03-19T01:22:50.710756Z",
     "iopub.status.idle": "2021-03-19T01:22:50.714390Z",
     "shell.execute_reply": "2021-03-19T01:22:50.713973Z"
    },
    "id": "tVN1QqFRDHBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxsiYnf2DN8K"
   },
   "source": [
    "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.718837Z",
     "iopub.status.busy": "2021-03-19T01:22:50.718258Z",
     "iopub.status.idle": "2021-03-19T01:22:50.722142Z",
     "shell.execute_reply": "2021-03-19T01:22:50.721679Z"
    },
    "id": "7QPzwWvSEwIp"
   },
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x1)\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.softplus(x1)\n",
    "  y = y0 + y1\n",
    "  ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRduLbE1H2IJ"
   },
   "source": [
    "Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.726408Z",
     "iopub.status.busy": "2021-03-19T01:22:50.725517Z",
     "iopub.status.idle": "2021-03-19T01:22:50.729769Z",
     "shell.execute_reply": "2021-03-19T01:22:50.729247Z"
    },
    "id": "e6GM-3evH1Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g1nKB6P-OnA"
   },
   "source": [
    "## Intermediate results\n",
    "\n",
    "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.734771Z",
     "iopub.status.busy": "2021-03-19T01:22:50.733754Z",
     "iopub.status.idle": "2021-03-19T01:22:50.737344Z",
     "shell.execute_reply": "2021-03-19T01:22:50.736899Z"
    },
    "id": "7XaPRAwUyYms"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dx = 2 * y, where y = x ** 2\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISkXuY7YzIcS"
   },
   "source": [
    "By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.741719Z",
     "iopub.status.busy": "2021-03-19T01:22:50.741071Z",
     "iopub.status.idle": "2021-03-19T01:22:50.745659Z",
     "shell.execute_reply": "2021-03-19T01:22:50.746022Z"
    },
    "id": "zZaCm3-9zVCi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
    "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.749607Z",
     "iopub.status.busy": "2021-03-19T01:22:50.748988Z",
     "iopub.status.idle": "2021-03-19T01:22:50.751260Z",
     "shell.execute_reply": "2021-03-19T01:22:50.751680Z"
    },
    "id": "j8bv_jQFg6CN"
   },
   "outputs": [],
   "source": [
    "del tape   # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ZY-9BUB7vX"
   },
   "source": [
    "## Notes on performance\n",
    "\n",
    "* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
    "\n",
    "* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
    "\n",
    "  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dLBpZsJebFq"
   },
   "source": [
    "## Gradients of non-scalar targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pldU9F5duP2"
   },
   "source": [
    "A gradient is fundamentally an operation on a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.756995Z",
     "iopub.status.busy": "2021-03-19T01:22:50.756351Z",
     "iopub.status.idle": "2021-03-19T01:22:50.761446Z",
     "shell.execute_reply": "2021-03-19T01:22:50.760975Z"
    },
    "id": "qI0sDV_WeXBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COEyYp34fxj4"
   },
   "source": [
    "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
    "\n",
    "* The gradient of the sum of the targets, or equivalently\n",
    "* The sum of the gradients of each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.765972Z",
     "iopub.status.busy": "2021-03-19T01:22:50.765297Z",
     "iopub.status.idle": "2021-03-19T01:22:50.769183Z",
     "shell.execute_reply": "2021-03-19T01:22:50.769576Z"
    },
    "id": "o4a6_YOcfWKS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvP-mkBMgbym"
   },
   "source": [
    "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.774011Z",
     "iopub.status.busy": "2021-03-19T01:22:50.773407Z",
     "iopub.status.idle": "2021-03-19T01:22:50.777405Z",
     "shell.execute_reply": "2021-03-19T01:22:50.777803Z"
    },
    "id": "DArPWqsSh5un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flDbx68Zh5Lb"
   },
   "source": [
    "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
    "\n",
    "If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwFswok8RAly"
   },
   "source": [
    "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.782532Z",
     "iopub.status.busy": "2021-03-19T01:22:50.781949Z",
     "iopub.status.idle": "2021-03-19T01:22:50.790536Z",
     "shell.execute_reply": "2021-03-19T01:22:50.790983Z"
    },
    "id": "JQvk_jnMmTDS"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:50.810395Z",
     "iopub.status.busy": "2021-03-19T01:22:50.803457Z",
     "iopub.status.idle": "2021-03-19T01:22:51.055330Z",
     "shell.execute_reply": "2021-03-19T01:22:51.055955Z"
    },
    "id": "e_f2QgDPmcPE"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kADybtQzYj4"
   },
   "source": [
    "## Control flow\n",
    "\n",
    "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n",
    "\n",
    "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.062698Z",
     "iopub.status.busy": "2021-03-19T01:22:51.062006Z",
     "iopub.status.idle": "2021-03-19T01:22:51.066812Z",
     "shell.execute_reply": "2021-03-19T01:22:51.066299Z"
    },
    "id": "ciFLizhrrjy7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKnLaiapsjeP"
   },
   "source": [
    "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
    "\n",
    "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.071362Z",
     "iopub.status.busy": "2021-03-19T01:22:51.070643Z",
     "iopub.status.idle": "2021-03-19T01:22:51.072987Z",
     "shell.execute_reply": "2021-03-19T01:22:51.073393Z"
    },
    "id": "8k05WmuAwPm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egypBxISAHhx"
   },
   "source": [
    "## Getting a gradient of `None`\n",
    "\n",
    "When a target is not connected to a source you will get a gradient of `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.077800Z",
     "iopub.status.busy": "2021-03-19T01:22:51.077181Z",
     "iopub.status.idle": "2021-03-19T01:22:51.081328Z",
     "shell.execute_reply": "2021-03-19T01:22:51.080841Z"
    },
    "id": "CU185WDM81Ut"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbKpHfBRJym"
   },
   "source": [
    "Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHDzDOiQ8xmw"
   },
   "source": [
    "### 1. Replaced a variable with a tensor\n",
    "\n",
    "In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
    "\n",
    "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.086395Z",
     "iopub.status.busy": "2021-03-19T01:22:51.085557Z",
     "iopub.status.idle": "2021-03-19T01:22:51.090278Z",
     "shell.execute_reply": "2021-03-19T01:22:51.089801Z"
    },
    "id": "QPKY4Tn9zX7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "EagerTensor : None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = x+1\n",
    "\n",
    "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "  x = x + 1   # This should be `x.assign_add(1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gwZKxgA97an"
   },
   "source": [
    "### 2. Did calculations outside of TensorFlow\n",
    "\n",
    "The tape can't record the gradient path if the calculation exits TensorFlow.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.095376Z",
     "iopub.status.busy": "2021-03-19T01:22:51.094577Z",
     "iopub.status.idle": "2021-03-19T01:22:51.098167Z",
     "shell.execute_reply": "2021-03-19T01:22:51.098504Z"
    },
    "id": "jmoLCDJb_yw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x2 = x**2\n",
    "\n",
    "  # This step is calculated with NumPy\n",
    "  y = np.mean(x2, axis=0)\n",
    "\n",
    "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "  # using `tf.convert_to_tensor`.\n",
    "  y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3YVfP3R-tp7"
   },
   "source": [
    "### 3. Took gradients through an integer or string\n",
    "\n",
    "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
    "\n",
    "Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.103946Z",
     "iopub.status.busy": "2021-03-19T01:22:51.103117Z",
     "iopub.status.idle": "2021-03-19T01:22:51.108993Z",
     "shell.execute_reply": "2021-03-19T01:22:51.108487Z"
    },
    "id": "9jlHXHqfASU3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsdP_mTHX9L1"
   },
   "source": [
    "TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyAZ7C8qCEs6"
   },
   "source": [
    "### 4. Took gradients through a stateful object\n",
    "\n",
    "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
    "\n",
    "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n",
    "\n",
    "A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.113936Z",
     "iopub.status.busy": "2021-03-19T01:22:51.113311Z",
     "iopub.status.idle": "2021-03-19T01:22:51.117358Z",
     "shell.execute_reply": "2021-03-19T01:22:51.117693Z"
    },
    "id": "C1tLeeRFE479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Update x1 = x1 + x0.\n",
    "  x1.assign_add(x0)\n",
    "  # The tape starts recording from x1.\n",
    "  y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKA92-dqF2r-"
   },
   "source": [
    "Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHvcDGIbOj2I"
   },
   "source": [
    "## No gradient registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoc-A6AxVqry"
   },
   "source": [
    "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
    "\n",
    "The `tf.raw_ops` page shows which low-level ops have gradients registered.\n",
    "\n",
    "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n",
    "\n",
    "For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.122994Z",
     "iopub.status.busy": "2021-03-19T01:22:51.122410Z",
     "iopub.status.idle": "2021-03-19T01:22:51.126621Z",
     "shell.execute_reply": "2021-03-19T01:22:51.127028Z"
    },
    "id": "HSb20FXc_V0U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(image, delta)\n",
    "\n",
    "try:\n",
    "  print(tape.gradient(new_image, [image, delta]))\n",
    "  assert False   # This should not happen.\n",
    "except LookupError as e:\n",
    "  print(f'{type(e).__name__}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoutjzATiEm"
   },
   "source": [
    "If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCTwc_dQXp2W"
   },
   "source": [
    "## Zeros instead of None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYDrVogA89eA"
   },
   "source": [
    "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T01:22:51.131767Z",
     "iopub.status.busy": "2021-03-19T01:22:51.131053Z",
     "iopub.status.idle": "2021-03-19T01:22:51.135770Z",
     "shell.execute_reply": "2021-03-19T01:22:51.135265Z"
    },
    "id": "U6zxk1sf9Ixx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "autodiff.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "pycharm-e707f7a0",
   "language": "python",
   "display_name": "PyCharm (handson-ml2)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}